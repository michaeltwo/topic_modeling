{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f5d38c0-76dd-4d49-a209-3804154a8628",
   "metadata": {},
   "source": [
    "### Import LIbraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "8450c3e8-54d8-41e9-820e-60dc9f2a4aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import wordnet\n",
    "from nltk import pos_tag\n",
    "import emoji\n",
    "import nltk\n",
    "from gensim.models import Phrases\n",
    "from gensim.models.phrases import Phraser\n",
    "from nltk.corpus import words\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3aaf74d-dda6-458c-b38b-d0275e6eedf5",
   "metadata": {},
   "source": [
    "### Load Tweets for Preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "4123e652-2353-4164-9123-fe5913beb9a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 398670 entries, 0 to 398669\n",
      "Data columns (total 1 columns):\n",
      " #   Column  Non-Null Count   Dtype \n",
      "---  ------  --------------   ----- \n",
      " 0   text    395561 non-null  object\n",
      "dtypes: object(1)\n",
      "memory usage: 3.0+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('metoo_tweets_dec2017.csv',usecols=[1])\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec6b9bf-a670-4e2d-be3e-ac15b94d6891",
   "metadata": {},
   "source": [
    "### Filter Tweets taht don't have #METOO tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "67c3e04d-3296-41ff-9b0b-70fe2917c512",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text\n",
      "0    American Harem.. #MeToo https://t.co/HjExLJdGuF\n",
      "1  @johnconyersjr  @alfranken  why have you guys ...\n",
      "3  Women have been talking about this crap the en...\n",
      "4  .@BetteMidler please speak to this sexual assa...\n",
      "5  We can't keep turning a blind eye and pretend ...\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 338674 entries, 0 to 398669\n",
      "Data columns (total 1 columns):\n",
      " #   Column  Non-Null Count   Dtype \n",
      "---  ------  --------------   ----- \n",
      " 0   text    338674 non-null  object\n",
      "dtypes: object(1)\n",
      "memory usage: 5.2+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "df = df.dropna(subset=['text']) # Drop NaN rows. \n",
    "#df['text'] = df['text'].str.lower() # Convert to lower case\n",
    "df = df[df['text'].str.contains('metoo', case=False, na=False)] # Filter rows that have #metoo and drop others. \n",
    "print(df.head())\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c851cdd-4d7d-48b9-84b3-c7ce26cd9d33",
   "metadata": {},
   "source": [
    "#### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "af6758a5-8217-4eac-a83b-1a6e66453a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_hashtag(hashtag):\n",
    "    # Split based on capitalization patterns\n",
    "    return ' '.join(re.findall(r'[A-Z]?[a-z]+|[A-Z]+(?![a-z])', hashtag))\n",
    "\n",
    "df = df.drop_duplicates(subset=['text']).reset_index(drop=True) # remove duplicates\n",
    "df['text'] = df['text'].apply(lambda x: re.sub(r'(\\w)(#)', r'\\1 \\2', x))\n",
    "df['text'] = df['text'].apply(lambda x: re.sub(r'#metoo', '', x, flags=re.IGNORECASE)) # Remove MeToo hashtag\n",
    "\n",
    "df['text'] = df['text'].apply(lambda x:  # Find hashtags and split them into words\n",
    "    ' '.join([split_hashtag(word[1:]) if word.startswith('#') else word for word in x.split()])\n",
    ")\n",
    "df['text'] = df['text'].str.lower() # lower case for easier processing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "3358f366-5877-4886-bbda-9c20db598fe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 138179 entries, 0 to 138178\n",
      "Data columns (total 1 columns):\n",
      " #   Column  Non-Null Count   Dtype \n",
      "---  ------  --------------   ----- \n",
      " 0   text    138179 non-null  object\n",
      "dtypes: object(1)\n",
      "memory usage: 1.1+ MB\n",
      "None\n",
      "                                                text\n",
      "0           american harem.. https://t.co/hjexljdguf\n",
      "1  @johnconyersjr @alfranken why have you guys no...\n",
      "2  women have been talking about this crap the en...\n",
      "3  .@bettemidler please speak to this sexual assa...\n",
      "4  we can't keep turning a blind eye and pretend ...\n"
     ]
    }
   ],
   "source": [
    "print(df.info())\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "b89f6d23-9b08-43ff-aa88-7fa535cb2cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply transformations to the 'text' column\n",
    "df['text'] = df['text'].apply(lambda x: \n",
    "    emoji.demojize( # Convert emojis to words\n",
    "        re.sub(r'[^a-zA-Z\\s]', '', # REMOVE punctuation and numbers \n",
    "                      re.sub(r\"\\bmetoo\\b\", '', # REMOVE word metoo\n",
    "                             re.sub(r\"@\\w+\", '', # REMOVE mentions\n",
    "                                    re.sub(r'http\\S+|www\\S+|https\\S+', '',x) # REMOVE URLs\n",
    "                                   \n",
    "                            )\n",
    "                     )\n",
    "              )\n",
    "    )\n",
    ")\n",
    "\n",
    "df['text'] = df['text'].apply(lambda x: re.sub(r'#', '', x)) # Remove hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "5ad1c1a5-6d35-47c4-9346-50a0bb29ea8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 138179 entries, 0 to 138178\n",
      "Data columns (total 1 columns):\n",
      " #   Column  Non-Null Count   Dtype \n",
      "---  ------  --------------   ----- \n",
      " 0   text    138179 non-null  object\n",
      "dtypes: object(1)\n",
      "memory usage: 1.1+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "97de1d24-99c2-4db9-a231-d23378cb65be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                     text\n",
      "138174  chief justice john roberts orders misconduct r...\n",
      "138175   need to start tweeting after this game ja xvs...\n",
      "138176            what microsoft learned from our moment \n",
      "138177   say victims of sexual harassment in japan via...\n",
      "138178  chief justice john roberts orders misconduct r...\n"
     ]
    }
   ],
   "source": [
    "print(df.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f10a70-a469-44f0-a3a8-b845c2ac5504",
   "metadata": {},
   "source": [
    "### Further processing tweets for Topic Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "ab2ecc3d-8ef8-4469-9f16-1fa05a431d1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\LENOVO\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Tokenizing the words\n",
    "nltk.download('punkt_tab')\n",
    "df['text'] = df['text'].apply(word_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "0cb18805-d71e-469d-9ae4-9f1e13a9ff39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text\n",
      "0                                  [american, harem]\n",
      "1  [why, have, you, guy, not, resigned, yet, libe...\n",
      "2  [woman, have, been, talking, about, this, crap...\n",
      "3  [please, speak, to, this, sexual, assault, by,...\n",
      "4  [we, cant, keep, turning, a, blind, eye, and, ...\n"
     ]
    }
   ],
   "source": [
    "# Lemmatization to put words in its origin\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "df['text'] = df['text'].apply(lambda x: [lemmatizer.lemmatize(word) for word in x])\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "0c375225-f18a-4b91-b6f9-a1a430a617bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text\n",
      "0                                  [american, harem]\n",
      "1           [guy, resigned, yet, liberal, hypocrisy]\n",
      "2  [woman, talking, crap, entire, time, finally, ...\n",
      "3        [please, speak, sexual, assault, interview]\n",
      "4  [cant, keep, turning, blind, eye, pretend, isn...\n"
     ]
    }
   ],
   "source": [
    "# Remove Stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "df['text'] = df['text'].apply(lambda x: [word for word in x if word not in stop_words])\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "3dc22ff5-ed5d-490f-8fb4-a2140afd078e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text\n",
      "0                                  [american, harem]\n",
      "1           [guy, resigned, yet, liberal, hypocrisy]\n",
      "2  [woman, talking, crap, entire, time, finally, ...\n",
      "3        [please, speak, sexual, assault, interview]\n",
      "4  [cant, keep, turning, blind, eye, pretend, isn...\n"
     ]
    }
   ],
   "source": [
    "# Remove short words \n",
    "df['text'] = df['text'].apply(lambda x: [word for word in x if len(word) > 2])\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8d0e071b-13ed-4275-a19f-3cdf73a84021",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text\n",
      "0                                         [american]\n",
      "1           [guy, resigned, yet, liberal, hypocrisy]\n",
      "2  [woman, talking, crap, entire, time, finally, ...\n",
      "3        [please, speak, sexual, assault, interview]\n",
      "4  [cant, keep, turning, blind, eye, pretend, isn...\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Flatten the list of all tokenized words in the 'text' column\n",
    "all_words = [word for tokens in df['text'] for word in tokens]\n",
    "word_counts = Counter(all_words)\n",
    "\n",
    "# Set a frequency threshold\n",
    "min_freq = 10 \n",
    "\n",
    "# Filter the words based on this minimum frequency\n",
    "df['text'] = df['text'].apply(lambda x: [word for word in x if word_counts[word] >= min_freq])\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb03455-8e7b-4dd6-bdbb-b0dcf4d83cfb",
   "metadata": {},
   "source": [
    "##  Process Single-Word Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "d82fa103-65b2-4e69-80c1-c2de79e5a3ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO\\miniconda3\\envs\\topicmodeling\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clusters:\n",
      "Cluster 3: sick, anyone, thats, feel, count, going, happened, damon, god, hell, trying, know, understand, cant, happen, anything, shes, away, answer, bullshit, sure, family, course, hand, try, ever, honest, shit, hug, love, used, something, everything, kid, life, nice, got, hey, really, shut, respect, sorry, told, sister, lady, hurt, funny, man, wish, raped, never, wonder, much, harassed, would, even, ill, exactly, well, worse, better, seriously, liberal, friend, actually, getting, damn, yeah, lost, sad, said, doe, wait, thinking, also, hate, joke, abused, everyone, thing, like, dude, girl, though, lol, bad, guess, hear, daughter, remember, mean, seems, done, youre, saying, still, fuck, gon, stay, guy, maybe, fucking, since, theyre, alone\n",
      "Cluster 16: number, pay, care, problem, truth, real, clear, happens, act, matter, hollywood, abuse, consent, report, everywhere, isnt, attention, case, workplace, bill, way, amp, accusation, stop, continue, must, shame, justice, end, linda, power, judge, claim, help, guilty, law, action, child, forget, money, harassment, silent, court, call, victim, accused, assault\n",
      "Cluster 4: rose, pound, leader, rally, rail, apology, mention, definitely, missed, spotlight, butt, hysteria, liked, interested, supporter, touched, nbc, leave, cash, spoke, tired, exposed, husband, included, eat, race, bitch, covering, trauma, result, indeed, perfect, csa, science, yoga, tweeted, asshole, month, wheres, japan, discus, expect, obviously, rise, turning, missing, selective, silenced, notice, heart, defend, doubt, includes, forgot, strike, goal, yea, btw, loser, disappointed, top, gay, horse, weapon, nfl, potus, kirsten, netflix, song, surprise, listening, racist, mary, defends, via, include, waiting, joy, double, rosearmy, drive, grab, movie, apologize, fine, appropriate, side, block, shocked, cat, meet, reply, hypocrite, interest, lock, sexualharassment, final, hashtags, honor, asking, suit, kidding, literally, quote, paying, new, danny, international, famous, icon, wanted, campus, standing, courtesy, played, conservative, second, talked, mostly, brilliant, kill, winter, regarding, mine, follow, correct, idiot, disgusted, outrage, fan, resigning, left, tag, anthem, bernie, saw, creepy, brother, beautiful, miss, congrats, monster, dead, pig, salute, omg, file, local, dog, update, liar, solidarity, anyway, thursday, dem, grabbed, pathetic, misogyny, birthday, andrew, reported, crazy, india, version, surprising, headline, photo, speaks, retweet, earth, bless, original, teaching, pastor, fantastic, loved, hijacked, weird, effect, democracy, bully, watching, topic, unacceptable, trigger, fox, body, meant, bos, happening, comment, home, agreed, weaponized, confused, whatnext, jail, moron, reminder, suck, free, awesome, check, green, worried, becoming, ben, bravery, steve, trouble, vile, unfortunately, ford, tonight, spirit, employee, memory, inspired, slutshaming, viral, december, disgrace, shocking, shaming, unwanted, disagree, published, congratulation, incident, cold, trending, military, bet, nope, club, imagine, moved, wall, post, army, resource, join, political, horrible, allegedly, worked, pick, worldwide, record, sassy, book, vagina, fraud, today, dare, pervert, immune, pissed, yep, crap, called, reaction, oped, grateful, poem, kiss, spot, catch, fire, dream, absolutely, complicit, weigh, spoken, posted, soul, museum, feminism, ignored, excuse, total, cool, scream, except, gross, queen, follows, healing, cnn\n",
      "Cluster 18: wow, yet, word, true, see, porn, come, one, look, read, many, may, thread, list, already, hope, coming, soon, made, experience, ive, heard, step, tweet, late, whats, first, next, disgusting, best, seen, weinstein, another, thought, amazing, write, surprised, day, hashtag, two\n",
      "Cluster 0: lite, complain, blow, rated, uhuh, ripple, xoxo, delegitimizing, lawrence, mate, familiar, smoke, statistic, sum, stalking, unfuckable, implication, notourmen, nurse, secondsource, accurate, convicted, fart, prez, finished, fuk, lmfao, farm, maggie, weaponizing, neither, cheer, blocked, sham, ting, deck, backer, curious, hussy, lucy, buddy, clause, brigade, misuse, lou, constitution, bombdiggity, sexuality, yup, mood, planet, hastag, fool, amen, consensual, stacia, ibelieveyou, hella, boundary, sculpture, lay, boom, charles, congo, slogan, mocked, believable, required, dealing, torn, muh, katie, sock, insanity, tickled, cked, correlative, buck, prediction, nightly, deer, kris, zinke, mayne, impactful, enjoyed, someday, cow, chelsea, hol, timely, antitrump, resigntrump, confirm, epitome, whowillyouhelp, caption, mum, lucky, unbelievable, whorefare, scumbag, independent, uhhuh, comin, ouch, salem, performs, soundtrack, chick, offended, pumped, vintage, horrifying, ghost, womenmen, pissing, filling, artworld, maria, mustread, heartbreaking, saved, disavow, flipped, heck, misleading, hang, purge, owns, observation, repercussion, appalling, shade, submission, assaulter, horrified, tricky, willingly, cancelled, chuck, revolting, perv, mail, sweet, journey, shameful, deplorables, fixed, bull, cancel, nutshell, investigated, perk, erm, jewish, awe, loose, harold, appropriating, preach, yikes, violation, olympia, bastard, sayin, geez, clint, whoelse, rofl, campaigning, goat, understands, superb, witchhunt, bravest, dax, inevitable, kappa, sickening, rain, pedo, mistletoe, lmao, pattern, fabrication, scary, germany, terrified, possession, untouchable, advertising, sorr, ron, shepersisted, ticktock, guaranteed, hack, flirt, yuck, despicable, kristen, gtgt, immigration, typical, rat, noticed, relate, element, unfollowing, istandwithasmi, bird, limit, november, syndrome, edited, bestie, objectify, livid, africa, mistaken, coward, dice, babysitter, thankyou, goodness, welp, iconic, railroaded, cousin, harmony, bro, whoa, convenient, nailed, ironically, summer, abo, epic, australia, gtfoh, diddled, scratch, rabbi, happensnow, shoe, aftermetoo, relatable, lick, pledged, keeler, hello, ditto, weighs, believewomen, fraudster, sniff, vortex, lastly, disappointing, fondled, supersize, payout, dying, oops, smashpatriarchy, shipping, vins, groundbreaking, visual, silently, musical, sup, horrendous, grade, exclusive, sistah, hows, painting, slap, emerged, denounce, fck, tim, asshat, substantial, lolol, disgust, gaslighting, responding, ahem, bioethics, vatican, nigga, paging, teeth, joseph, scare, sista, smdh, backfiring, austin, tbh, cult, watermelon, copyright, fab, disease, exploring, forgetting, throwing, weaponization, drastic, bot, christ, print, success, prostitute, puritanism, responded, express, master, housecleaning, classic, terrifying, avalanche, kaep, offensive, peak, embarrassment, sickens, hilarious, ordered, impeached, hysterical, nomoore, sigh, shakespeare\n",
      "Cluster 2: silence, brave, proud, thank, speaking\n",
      "Cluster 17: latest, news, thanks\n",
      "Cluster 10: vote, yes, get, agree, person, make, history, kaepernick, take, time, deserve, bring, choice, win, year, colin, cover, turn, finally, think, wrong, back, behind, voted, long, use, add, either, totally, put, dreamer\n",
      "Cluster 7: far\n",
      "Cluster 14: woman, people, men, male\n",
      "Cluster 13: fear, point, start, part, changing, article, wondering, great, created, beginning, continues, interesting, talking, work, beyond, backlash, ready, feminist, politics, revolution, powerful, excellent, working, important, issue, generation, change, become, talk, discussion, question, perspective, good\n",
      "Cluster 8: tell, say, believed, believe, support\n",
      "Cluster 6: movement, campaign\n",
      "Cluster 1: moment\n",
      "Cluster 9: chris, resign, trump, resist, maga, gop, franken, senator, democrat, gillibrand, accuser, scam, mueller, president\n",
      "Cluster 15: want, need, dont, let, please\n",
      "Cluster 5: stand, speak, survivor\n",
      "Cluster 19: right, march\n",
      "Cluster 12: moore\n",
      "Cluster 11: story\n",
      "\n",
      "Processed DataFrame:\n",
      "                                                     text\n",
      "0                                       [american, harem]\n",
      "1                [guy, resigned, yet, liberal, hypocrisy]\n",
      "2       [woman, talking, crap, entire, time, finally, ...\n",
      "3             [please, speak, sexual, assault, interview]\n",
      "4       [cant, keep, turning, blind, eye, pretend, isn...\n",
      "...                                                   ...\n",
      "135338                    [want, need, dont, let, please]\n",
      "135339                           [stand, speak, survivor]\n",
      "135340                                     [right, march]\n",
      "135341                                            [moore]\n",
      "135342                                            [story]\n",
      "\n",
      "[135343 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "# Filter out single-word documents\n",
    "single_word_docs = df[df['text'].apply(len) == 1]\n",
    "multi_word_docs = df[df['text'].apply(len) != 1]\n",
    "\n",
    "# Extract single words (retain duplicates across the dataset)\n",
    "single_words = [doc[0] for doc in single_word_docs['text']]\n",
    "\n",
    "# Train Word2Vec on multi-word documents\n",
    "multi_word_texts = multi_word_docs['text'].tolist()\n",
    "model = Word2Vec(sentences=multi_word_texts, vector_size=100, min_count=1, workers=4)\n",
    "\n",
    "# Filter single words to include only those present in the Word2Vec model's vocabulary\n",
    "single_words = [word for word in single_words if word in model.wv]\n",
    "\n",
    "# Get word embeddings for single words\n",
    "word_vectors = np.array([model.wv[word] for word in single_words])\n",
    "\n",
    "# Cluster single-word embeddings\n",
    "num_clusters = 20\n",
    "kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
    "kmeans.fit(word_vectors)\n",
    "\n",
    "# Group single words into clusters\n",
    "clusters = defaultdict(list)\n",
    "for i, word in enumerate(single_words):\n",
    "    clusters[kmeans.labels_[i]].append(word)\n",
    "\n",
    "# Remove duplicates within each cluster/document\n",
    "filtered_clusters = {cluster_id: list(set(words)) for cluster_id, words in clusters.items()}\n",
    "\n",
    "# Create new documents from filtered clusters\n",
    "new_documents = [\" \".join(words) for words in filtered_clusters.values()]\n",
    "\n",
    "# Combine new documents with the original multi-word documents\n",
    "final_documents = multi_word_texts + [doc.split() for doc in new_documents]\n",
    "\n",
    "# Update the DataFrame with the processed documents\n",
    "df_processed = pd.DataFrame({'text': final_documents})\n",
    "\n",
    "# Display the clusters and the processed DataFrame\n",
    "print(\"Clusters:\")\n",
    "for cluster_id, words in filtered_clusters.items():\n",
    "    print(f\"Cluster {cluster_id}: {', '.join(words)}\")\n",
    "\n",
    "print(\"\\nProcessed DataFrame:\")\n",
    "print(df_processed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac30b9de-ad2e-4b15-bbc4-76ba4b923c0a",
   "metadata": {},
   "source": [
    "### Save the preprocessed and tokenized words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "93d7652d-c17b-4c6a-a615-30f8d731aa90",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_processed.to_pickle('tokenized_tweets_lemmatzation.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d9ed36-1a74-42ea-a5bd-2a134c50eb90",
   "metadata": {},
   "source": [
    "### PORTER STEMMER "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d53ecc34-db63-4805-83ff-b87ba4408f79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text\n",
      "0                                  [american, harem]\n",
      "1               [guy, resign, yet, liber, hypocrisi]\n",
      "2  [women, talk, crap, entir, time, final, someon...\n",
      "3         [pleas, speak, sexual, assault, interview]\n",
      "4  [cant, keep, turn, blind, eye, pretend, isnt, ...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "stemmer = PorterStemmer() # Initialize Stemmer\n",
    "\n",
    "df['text'] = df['text'].apply(lambda x: [stemmer.stem(word) for word in x])  #Stemming every word\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c0f69d5b-05a5-4963-b30e-25c209af3b63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text\n",
      "0                                  [american, harem]\n",
      "1               [guy, resign, yet, liber, hypocrisi]\n",
      "2  [women, talk, crap, entir, time, final, someon...\n",
      "3         [pleas, speak, sexual, assault, interview]\n",
      "4  [cant, keep, turn, blind, eye, pretend, isnt, ...\n"
     ]
    }
   ],
   "source": [
    "# Remove short words and infrequent words\n",
    "df['text'] = df['text'].apply(lambda x: [word for word in x if len(word) > 2])\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "323e4d88-474d-49e8-831a-2ed4c96d6b5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text\n",
      "0                                  [american, harem]\n",
      "1               [guy, resign, yet, liber, hypocrisi]\n",
      "2  [women, talk, crap, entir, time, final, someon...\n",
      "3         [pleas, speak, sexual, assault, interview]\n",
      "4  [cant, keep, turn_blind, eye, pretend, isnt, r...\n"
     ]
    }
   ],
   "source": [
    "# Create N-Grams for words that don't make sense individually\n",
    "sentences = df['text'].tolist() # convert tokens back to sentences\n",
    "bigram = Phrases(sentences, min_count=5, threshold=100) # define phrases\n",
    "bigram_mod = Phraser(bigram) # initialize Phraser with bigram settings. \n",
    "df['text'] = df['text'].apply(lambda x: bigram_mod[x])\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4955a31b-df47-4e51-97f3-23ee76e27458",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_pickle('tokenized_tweets_stemmer.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c1b4ea05-aaab-4a8f-95f1-5eb7cf529b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_excel('tokenized_tweets_stemmer.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f84c649-2e55-4506-92a1-7094d04e969f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
