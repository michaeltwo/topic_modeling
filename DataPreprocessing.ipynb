{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f5d38c0-76dd-4d49-a209-3804154a8628",
   "metadata": {},
   "source": [
    "### Import LIbraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8450c3e8-54d8-41e9-820e-60dc9f2a4aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import emoji\n",
    "import nltk\n",
    "from gensim.models import Phrases\n",
    "from gensim.models.phrases import Phraser\n",
    "from nltk.corpus import words\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3aaf74d-dda6-458c-b38b-d0275e6eedf5",
   "metadata": {},
   "source": [
    "### Load Tweets for Preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4123e652-2353-4164-9123-fe5913beb9a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 398670 entries, 0 to 398669\n",
      "Data columns (total 1 columns):\n",
      " #   Column  Non-Null Count   Dtype \n",
      "---  ------  --------------   ----- \n",
      " 0   text    395561 non-null  object\n",
      "dtypes: object(1)\n",
      "memory usage: 3.0+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('metoo_tweets_dec2017.csv',usecols=[1])\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec6b9bf-a670-4e2d-be3e-ac15b94d6891",
   "metadata": {},
   "source": [
    "### Filter Tweets taht don't have #METOO tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67c3e04d-3296-41ff-9b0b-70fe2917c512",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text\n",
      "0    American Harem.. #MeToo https://t.co/HjExLJdGuF\n",
      "1  @johnconyersjr  @alfranken  why have you guys ...\n",
      "3  Women have been talking about this crap the en...\n",
      "4  .@BetteMidler please speak to this sexual assa...\n",
      "5  We can't keep turning a blind eye and pretend ...\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 338674 entries, 0 to 398669\n",
      "Data columns (total 1 columns):\n",
      " #   Column  Non-Null Count   Dtype \n",
      "---  ------  --------------   ----- \n",
      " 0   text    338674 non-null  object\n",
      "dtypes: object(1)\n",
      "memory usage: 5.2+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "df = df.dropna(subset=['text']) # Drop NaN rows. \n",
    "#df['text'] = df['text'].str.lower() # Convert to lower case\n",
    "df = df[df['text'].str.contains('metoo', case=False, na=False)]#df = df[df['text'].str.contains('#MeToo')] # Filter rows that have #metoo and drop others. \n",
    "print(df.head())\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c851cdd-4d7d-48b9-84b3-c7ce26cd9d33",
   "metadata": {},
   "source": [
    "#### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af6758a5-8217-4eac-a83b-1a6e66453a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_hashtag(hashtag):\n",
    "    # Split based on capitalization patterns\n",
    "    return ' '.join(re.findall(r'[A-Z]?[a-z]+|[A-Z]+(?![a-z])', hashtag))\n",
    "\n",
    "df = df.drop_duplicates(subset=['text']).reset_index(drop=True) # remove duplicates\n",
    "df['text'] = df['text'].apply(lambda x: re.sub(r'#metoo', '', x, flags=re.IGNORECASE)) # Remove MeToo hashtag\n",
    "\n",
    "df['text'] = df['text'].apply(lambda x:  # Find hashtags and split them into words\n",
    "    ' '.join([split_hashtag(word[1:]) if word.startswith('#') else word for word in x.split()])\n",
    ")\n",
    "df['text'] = df['text'].str.lower() # lower case for easier processing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b89f6d23-9b08-43ff-aa88-7fa535cb2cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply transformations to the 'text' column\n",
    "df['text'] = df['text'].apply(lambda x: \n",
    "    emoji.demojize( # Convert emojis to words\n",
    "        re.sub(r'[^a-zA-Z\\s]', '', # REMOVE punctuation and numbers \n",
    "                      re.sub(r\"\\bmetoo\\b\", '', # REMOVE word metoo\n",
    "                             re.sub(r\"@\\w+\", '', # REMOVE mentions\n",
    "                                    re.sub(r'http\\S+|www\\S+|https\\S+', '',x) # REMOVE URLs\n",
    "                                   \n",
    "                            )\n",
    "                     )\n",
    "              )\n",
    "    )\n",
    ")\n",
    "\n",
    "df['text'] = df['text'].apply(lambda x: re.sub(r'#', '', x)) # Remove hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3e3868cf-fc60-4ed8-86d7-6aa6d3bc455f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text    black lives matter with yup ok\n",
      "Name: 9, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df.iloc[9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5ad1c1a5-6d35-47c4-9346-50a0bb29ea8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 138179 entries, 0 to 138178\n",
      "Data columns (total 1 columns):\n",
      " #   Column  Non-Null Count   Dtype \n",
      "---  ------  --------------   ----- \n",
      " 0   text    138179 non-null  object\n",
      "dtypes: object(1)\n",
      "memory usage: 1.1+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5c61308b-ec67-4f8f-824d-21960b167ba9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 113383 entries, 0 to 113382\n",
      "Data columns (total 1 columns):\n",
      " #   Column  Non-Null Count   Dtype \n",
      "---  ------  --------------   ----- \n",
      " 0   text    113383 non-null  object\n",
      "dtypes: object(1)\n",
      "memory usage: 885.9+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "df = df.drop_duplicates(subset=['text']).reset_index(drop=True)\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "97de1d24-99c2-4db9-a231-d23378cb65be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                     text\n",
      "113378  rt  with the attacks on matt damon today the m...\n",
      "113379                                          wait oops\n",
      "113380  my bro claims is just a witchhunt and refuses ...\n",
      "113381   need to start tweeting after this game ja xvs...\n",
      "113382   say victims of sexual harassment in japan via...\n"
     ]
    }
   ],
   "source": [
    "print(df.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f10a70-a469-44f0-a3a8-b845c2ac5504",
   "metadata": {},
   "source": [
    "### Further processing tweets for Topic Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ab2ecc3d-8ef8-4469-9f16-1fa05a431d1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\LENOVO\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Tokenizing the words\n",
    "nltk.download('punkt_tab')\n",
    "df['text'] = df['text'].apply(word_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0c375225-f18a-4b91-b6f9-a1a430a617bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text\n",
      "0                                  [american, harem]\n",
      "1          [guys, resigned, yet, liberal, hypocrisy]\n",
      "2  [women, talking, crap, entire, time, finally, ...\n",
      "3        [please, speak, sexual, assault, interview]\n",
      "4  [cant, keep, turning, blind, eye, pretend, isn...\n"
     ]
    }
   ],
   "source": [
    "# Remove Stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "df['text'] = df['text'].apply(lambda x: [word for word in x if word not in stop_words])\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0cb18805-d71e-469d-9ae4-9f1e13a9ff39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text\n",
      "0                                  [american, harem]\n",
      "1           [guy, resigned, yet, liberal, hypocrisy]\n",
      "2  [woman, talking, crap, entire, time, finally, ...\n",
      "3        [please, speak, sexual, assault, interview]\n",
      "4  [cant, keep, turning, blind, eye, pretend, isn...\n"
     ]
    }
   ],
   "source": [
    "# Lemmatization to put words in its origin\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "df['text'] = df['text'].apply(lambda x: [lemmatizer.lemmatize(word) for word in x])\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3dc22ff5-ed5d-490f-8fb4-a2140afd078e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text\n",
      "0                                  [american, harem]\n",
      "1           [guy, resigned, yet, liberal, hypocrisy]\n",
      "2  [woman, talking, crap, entire, time, finally, ...\n",
      "3        [please, speak, sexual, assault, interview]\n",
      "4  [cant, keep, turning, blind, eye, pretend, isn...\n"
     ]
    }
   ],
   "source": [
    "# Remove short words and infrequent words\n",
    "df['text'] = df['text'].apply(lambda x: [word for word in x if len(word) > 2])\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8d0e071b-13ed-4275-a19f-3cdf73a84021",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text\n",
      "0                                         [american]\n",
      "1           [guy, resigned, yet, liberal, hypocrisy]\n",
      "2  [woman, talking, crap, entire, time, finally, ...\n",
      "3        [please, speak, sexual, assault, interview]\n",
      "4  [cant, keep, turning, blind, eye, pretend, isn...\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Flatten the list of all tokenized words in the 'text' column\n",
    "all_words = [word for tokens in df['text'] for word in tokens]\n",
    "word_counts = Counter(all_words)\n",
    "\n",
    "# Set a frequency threshold\n",
    "min_freq = 10 \n",
    "\n",
    "# Filter the words based on this minimum frequency\n",
    "df['text'] = df['text'].apply(lambda x: [word for word in x if word_counts[word] >= min_freq])\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d82fa103-65b2-4e69-80c1-c2de79e5a3ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text\n",
      "0                                  [american, harem]\n",
      "1           [guy, resigned, yet, liberal, hypocrisy]\n",
      "2  [woman, talking, crap, entire, time, finally, ...\n",
      "3        [please, speak, sexual, assault, interview]\n",
      "4  [cant, keep, turning, blind_eye, pretend, isnt...\n"
     ]
    }
   ],
   "source": [
    "# Create N-Grams for words that don't make sense individually\n",
    "sentences = df['text'].tolist() # convert tokens back to sentences\n",
    "bigram = Phrases(sentences, min_count=5, threshold=100) # define phrases\n",
    "bigram_mod = Phraser(bigram) # initialize Phraser with bigram settings. \n",
    "df['text'] = df['text'].apply(lambda x: bigram_mod[x])\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac30b9de-ad2e-4b15-bbc4-76ba4b923c0a",
   "metadata": {},
   "source": [
    "### Save the preprocessed and tokenized words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "93d7652d-c17b-4c6a-a615-30f8d731aa90",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_pickle('tokenized_tweets_lemmatzation.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d9ed36-1a74-42ea-a5bd-2a134c50eb90",
   "metadata": {},
   "source": [
    "### PORTER STEMMER "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d53ecc34-db63-4805-83ff-b87ba4408f79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text\n",
      "0                                  [american, harem]\n",
      "1               [guy, resign, yet, liber, hypocrisi]\n",
      "2  [women, talk, crap, entir, time, final, someon...\n",
      "3         [pleas, speak, sexual, assault, interview]\n",
      "4  [cant, keep, turn, blind, eye, pretend, isnt, ...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "stemmer = PorterStemmer() # Initialize Stemmer\n",
    "\n",
    "df['text'] = df['text'].apply(lambda x: [stemmer.stem(word) for word in x])  #Stemming every word\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c0f69d5b-05a5-4963-b30e-25c209af3b63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text\n",
      "0                                  [american, harem]\n",
      "1               [guy, resign, yet, liber, hypocrisi]\n",
      "2  [women, talk, crap, entir, time, final, someon...\n",
      "3         [pleas, speak, sexual, assault, interview]\n",
      "4  [cant, keep, turn, blind, eye, pretend, isnt, ...\n"
     ]
    }
   ],
   "source": [
    "# Remove short words and infrequent words\n",
    "df['text'] = df['text'].apply(lambda x: [word for word in x if len(word) > 2])\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "323e4d88-474d-49e8-831a-2ed4c96d6b5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text\n",
      "0                                  [american, harem]\n",
      "1               [guy, resign, yet, liber, hypocrisi]\n",
      "2  [women, talk, crap, entir, time, final, someon...\n",
      "3         [pleas, speak, sexual, assault, interview]\n",
      "4  [cant, keep, turn_blind, eye, pretend, isnt, r...\n"
     ]
    }
   ],
   "source": [
    "# Create N-Grams for words that don't make sense individually\n",
    "sentences = df['text'].tolist() # convert tokens back to sentences\n",
    "bigram = Phrases(sentences, min_count=5, threshold=100) # define phrases\n",
    "bigram_mod = Phraser(bigram) # initialize Phraser with bigram settings. \n",
    "df['text'] = df['text'].apply(lambda x: bigram_mod[x])\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4955a31b-df47-4e51-97f3-23ee76e27458",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_pickle('tokenized_tweets_stemmer.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c1b4ea05-aaab-4a8f-95f1-5eb7cf529b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_excel('tokenized_tweets_stemmer.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f84c649-2e55-4506-92a1-7094d04e969f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
