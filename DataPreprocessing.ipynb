{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f5d38c0-76dd-4d49-a209-3804154a8628",
   "metadata": {},
   "source": [
    "### Import LIbraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "8450c3e8-54d8-41e9-820e-60dc9f2a4aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import emoji\n",
    "import nltk\n",
    "from gensim.models import Phrases\n",
    "from gensim.models.phrases import Phraser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3aaf74d-dda6-458c-b38b-d0275e6eedf5",
   "metadata": {},
   "source": [
    "### Load Tweets for Preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "4123e652-2353-4164-9123-fe5913beb9a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 398670 entries, 0 to 398669\n",
      "Data columns (total 1 columns):\n",
      " #   Column  Non-Null Count   Dtype \n",
      "---  ------  --------------   ----- \n",
      " 0   text    395561 non-null  object\n",
      "dtypes: object(1)\n",
      "memory usage: 3.0+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('metoo_tweets_dec2017.csv',usecols=[1])\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec6b9bf-a670-4e2d-be3e-ac15b94d6891",
   "metadata": {},
   "source": [
    "### Filter Tweets taht don't have #METOO tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "67c3e04d-3296-41ff-9b0b-70fe2917c512",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text\n",
      "0    american harem.. #metoo https://t.co/hjexljdguf\n",
      "1  @johnconyersjr  @alfranken  why have you guys ...\n",
      "3  women have been talking about this crap the en...\n",
      "4  .@bettemidler please speak to this sexual assa...\n",
      "5  we can't keep turning a blind eye and pretend ...\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 338554 entries, 0 to 398669\n",
      "Data columns (total 1 columns):\n",
      " #   Column  Non-Null Count   Dtype \n",
      "---  ------  --------------   ----- \n",
      " 0   text    338554 non-null  object\n",
      "dtypes: object(1)\n",
      "memory usage: 5.2+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "df = df.dropna(subset=['text']) # Drop NaN rows. \n",
    "df['text'] = df['text'].str.lower() # Convert to lower case\n",
    "df = df[df['text'].str.contains('#metoo')] # Filter rows that have #metoo and drop others. \n",
    "print(df.head())\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c851cdd-4d7d-48b9-84b3-c7ce26cd9d33",
   "metadata": {},
   "source": [
    "#### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "7e101dc5-ab06-453a-921f-d086d687b9a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text\n",
      "0                              american harem metoo \n",
      "1      why have you guys not resigned yet liberal...\n",
      "3  women have been talking about this crap the en...\n",
      "4   please speak to this sexual assault by  durin...\n",
      "5  we cant keep turning a blind eye and pretend t...\n"
     ]
    }
   ],
   "source": [
    "df['text'] = df['text'].apply(lambda x: \n",
    "    emoji.demojize( # Convert emojis to words\n",
    "        re.sub(r'[^a-zA-Z\\s]', '', # REMOVE punctuation and numbers\n",
    "               re.sub(r\"#metoo\", '', # REMOVE #metoo hashtag \n",
    "                      re.sub(r\"#\", '', # REMOVE other hashtags but keep words\n",
    "                             re.sub(r\"@\\w+\", '', # REMOVE mentions\n",
    "                                    re.sub(r'http\\S+|www\\S+|https\\S+', '', x) # REMOVE URLs\n",
    "                                   )\n",
    "                            )\n",
    "                     )\n",
    "              )\n",
    "    )\n",
    ")\n",
    "print(df.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f10a70-a469-44f0-a3a8-b845c2ac5504",
   "metadata": {},
   "source": [
    "### Further processing tweets for Topic Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "ab2ecc3d-8ef8-4469-9f16-1fa05a431d1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\LENOVO\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n"
     ]
    }
   ],
   "source": [
    "# Tokenizing the words\n",
    "nltk.download('punkt_tab')\n",
    "df['text'] = df['text'].apply(word_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "0c375225-f18a-4b91-b6f9-a1a430a617bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text\n",
      "0                           [american, harem, metoo]\n",
      "1   [guys, resigned, yet, liberal, hypocrisy, metoo]\n",
      "3  [women, talking, crap, entire, time, finally, ...\n",
      "4  [please, speak, sexual, assault, interview, me...\n",
      "5  [cant, keep, turning, blind, eye, pretend, isn...\n"
     ]
    }
   ],
   "source": [
    "# Remove Stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "df['text'] = df['text'].apply(lambda x: [word for word in x if word not in stop_words])\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "0cb18805-d71e-469d-9ae4-9f1e13a9ff39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text\n",
      "0                           [american, harem, metoo]\n",
      "1    [guy, resigned, yet, liberal, hypocrisy, metoo]\n",
      "3  [woman, talking, crap, entire, time, finally, ...\n",
      "4  [please, speak, sexual, assault, interview, me...\n",
      "5  [cant, keep, turning, blind, eye, pretend, isn...\n"
     ]
    }
   ],
   "source": [
    "# Lemmatization to put words in its origin\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "df['text'] = df['text'].apply(lambda x: [lemmatizer.lemmatize(word) for word in x])\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "3dc22ff5-ed5d-490f-8fb4-a2140afd078e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text\n",
      "0                           [american, harem, metoo]\n",
      "1    [guy, resigned, yet, liberal, hypocrisy, metoo]\n",
      "3  [woman, talking, crap, entire, time, finally, ...\n",
      "4  [please, speak, sexual, assault, interview, me...\n",
      "5  [cant, keep, turning, blind, eye, pretend, isn...\n"
     ]
    }
   ],
   "source": [
    "# Remove short words and infrequent words\n",
    "df['text'] = df['text'].apply(lambda x: [word for word in x if len(word) > 2])\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "d82fa103-65b2-4e69-80c1-c2de79e5a3ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text\n",
      "0                           [american, harem, metoo]\n",
      "1    [guy, resigned, yet, liberal, hypocrisy, metoo]\n",
      "3  [woman, talking, crap, entire, time, finally, ...\n",
      "4  [please, speak, sexual, assault, interview, me...\n",
      "5  [cant, keep, turning, blind_eye, pretend, isnt...\n"
     ]
    }
   ],
   "source": [
    "# Create N-Grams for words that don't make sense individually\n",
    "sentences = df['text'].tolist() # convert tokens back to sentences\n",
    "bigram = Phrases(sentences, min_count=5, threshold=100) # define phrases\n",
    "bigram_mod = Phraser(bigram) # initialize Phraser with bigram settings. \n",
    "df['text'] = df['text'].apply(lambda x: bigram_mod[x])\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac30b9de-ad2e-4b15-bbc4-76ba4b923c0a",
   "metadata": {},
   "source": [
    "### Save the preprocessed and tokenized words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "93d7652d-c17b-4c6a-a615-30f8d731aa90",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_pickle('tokenized_tweets.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d53ecc34-db63-4805-83ff-b87ba4408f79",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
